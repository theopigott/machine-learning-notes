\documentclass[a4paper,12pt]{article}

\usepackage{amsthm, amssymb, amsmath}
\usepackage{mathtools}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{graphicx, color}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{tikz}

\theoremstyle{definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{prop}{Proposition}
\newtheorem*{cor}{Corollary}
\newtheorem*{defn}{Definition}
\newtheorem*{rem}{Remark}
\newtheorem*{ex}{Example}
\newtheorem*{notn}{Notation}

\setenumerate{label=(\roman*)}

%probability
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
%blackboard sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
%set
\providecommand\given{} % so it exists
\newcommand\SetSymbol[1][]{
   \nonscript\,#1: \allowbreak \nonscript\,\mathopen{}}
\DeclarePairedDelimiterX\Set[1]{\lbrace}{\rbrace}%
 { \renewcommand\given{\SetSymbol[\delimsize]} #1 }
\renewcommand{\given}{\mid}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} % for vectors of Greek letters
\newcommand{\grad}[1]{\gv{\nabla} #1} % for gradient
\let\divsymb=\div % rename builtin command \div to \divsymb
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\title{Machine Learning}
\author{Lectured by Andrew Ng on Coursera}
\date{2017}

\usepackage{fancyhdr}
\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \fancyfoot[L]{\small{\emph{Notes by Theo Pigott}}}% Left footer
  \fancyfoot[R]{\thepage}
}
\pagestyle{plain}

\begin{document}
\maketitle

\section{Introduction}

\begin{defn}[Machine Learning - Tom Mitchell, 1998]
A computer program is said to \emph{learn} from experience $E$ with respect to
some task $T$ and some performance measure $P$, if its performance on $T$,
as measured by $P$, improves with experience $E$.
\end{defn}

\section{Linear Regression}

\begin{notn}[Training data]
We have a set of training examples, denoted by $x \in \R^{n} \times \R^{m}$ ($m$ examples of $n$ features). Concretely, $x^{(i)}$ denotes the features of the $i^{th}$ training example, with $x^{(i)}_{j}$ being the value of feature $j$ in the $i^{th}$ training example. 

Conventionally, we take an extra row of ones as the $0^{th}$ feature ($x_0 := 1$) and denote the corresponding $(m+1) \times n$ matrix $X$ the \emph{design matrix}. 

The output variable is $y \in \R^{m}$.
\end{notn}

\begin{defn}[Linear hypothesis]
Our hypothesis is 
\[
h_{\theta}(x) = \theta^T X
\] where $\theta \in \R^{n+1}$ is a vector of parameters to be estimated.
\end{defn}

\begin{defn}[Squared Loss Cost Function]
The \emph{cost function} 
\begin{align*}
J(\theta) &= \frac{1}{2m} \sum_{i=1}^{m}{ \left( h_{\theta}(x^{i}) - y^{i}\right)^{2} } \\
&= \frac{1}{2m} (\theta^{T} X - y)^{T} (\theta^{T} X - y) 
\end{align*}
represents a measure of the fit of a particular choice of parameters $\theta$.
\end{defn}

We estimate $\theta$ as the value minimising $J(\theta)$.

\subsection{Gradient Descent}
\emph{Gradient descent} involves following the gradient of this cost function to find its minimum. In practice, taking an initial estimate $\theta_0$ and \emph{learning rate} $\alpha$ we iteratively compute
\[
\theta := \theta - \alpha \grad J
\]
that is
\[
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
\]
for $j \in \Set{0, \ldots, n+1}$.

For linear regression,
\begin{align*}
\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{1}{m} \sum_{i=1}^{m}{ \left( h_{\theta}(x^{i}) - y^{i}\right) x^{(i)}_j } \\
&= \frac{1}{m} X (\theta^T X - y)
\end{align*}


We declare convergence when $J(\theta)$ decreases by less than $\epsilon$ in a single iteration.

The learning rate $\alpha$ must be chosen carefully. Too large and convergence may not occur, too small and convergence will be slow.

\subsubsection{Feature scaling}
We can speed up gradient descent via \emph{feature scaling} and \emph{mean normalization}. Intuitively, this involves scaling each feature so that the steps taken along the gradient are roughly uniform across the dimensions of the feature space. Concretely, we set
\[
x = \frac{x - \mu}{\sigma}
\]
where $\mu$ is the (row-wise) mean of $x$ and $\sigma$ is the (row-wise) standard deviation of $x$.

\subsection{Analytic solution}
The minimum of the cost function can also be found analytically (solve the system of equations $\grad J = 0$ to obtain the exact solution
\[
\theta = (X^T X)^{-1} X^T y
\]
in the cast where $(X^T X)$ is invertible. Even in the singular case, we can take a numerical solution with the pseudo-inverse, e.g. via the Octave function \verb!pinv!. The singular case can occur when there are redundant (linearly dependent) features, or too many features ($m \leq n$).

\section{Logistic Regression - Classification}
We now restrict $y \in \Set{0, 1}^{m}$ so that we now have the problem of \emph{classifying} an observation $x$.

\begin{defn}[Classification hypothesis]
Our hypothesis is 
\begin{align*}
h_{\theta}(x) &= g(\theta^T x) \\
&= \frac{1}{1 + e^{-\theta^{T} x}}
\end{align*}
where $g(z)$ is the \emph{sigmoid} function, so that $0 \leq h_{\theta}(x) \leq 1$. We interpret
\begin{align*}
h_{\theta}(x) &= \Pr_{\theta}(y = 1 \given x) \\
&= \Pr(y = 1 \given x, \theta)
\end{align*}
and `predict' that $y = 1$ if $h_{\theta}(x) >= 0.5$, i.e. if $\theta^{T} x >= 0$. The surface $h_{\theta}(x) = 0.5$ is known as the \emph{decision boundary}.
\end{defn}

The squared loss cost function is not convex in the case of logistic regression, we instead use the logistic cost function. 
\begin{defn}[Logistic Cost function]
The \emph{logistic} cost function is
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathrm{Cost}(h_{\theta}(x^{(i)}), y^{(i)})
\]
where
\begin{align*}
\mathrm{Cost}(h_{\theta}(x), y) & = 
\begin{cases}
-\log(h_{\theta}(x)) & \text{for } y = 1 \\
-(1 - \log(h_{\theta}(x))) & \text{for } y = 0
\end{cases} \\
& = - y \log(h_{\theta}(x)) - (1 - y)(1 - \log(h_{\theta}(x)))
\end{align*}
\end{defn}

This form of cost function can be derived from \emph{maximum likelihood estimation} for the binomial distribution.

Gradient descent applies in the same way, and moreover we again find that
\[
\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m}{ \left( h_{\theta}(x^{i}) - y^{i}\right) x^{(i)}_j }
\]
though of course with the new hypothesis function.

\subsubsection{Advanced optimization}
There exist other, more complex algorithms to minimum the cost function, such as
\begin{enumerate}
\item Conjugate gradient
\item BFGS
\item L-BFGS
\end{enumerate}
which don't involve picking a learning rate and are often faster.

\subsection{Multiclass classification}
For the case of classifying $y \in \Set{1, \ldots, k}$ we can apply the principle of \emph{one-vs-all} to train $k$ classifiers $h_{\theta}^{(i)}$ each of which is predicting the probability $\Pr_{\theta}(y = i \given x)$ (against all other classes). The final prediction is then taken as the class $i$ maximising $h_{\theta}^{(i)}$.

\subsection{Addressing overfitting}
With too many features, the learned hypothesis may fit the training set very well $(J(\theta) \approx 0$ but fail to generalize to new examples. This is known as \emph{overfitting}.

This can be addressed by reducing the number of features (manually or alorithmically), or via regularization. 

\subsubsection{Regularization}
\label{logistic-regularization}
The idea is too keep all features, but reduce the magnitude of the parameters $\theta_j$. This works well when there are lots of features, each contributing a bit to predicting $y$.

To achieve this, we \emph{penalize} the parameters inside the cost function.

For \emph{linear regression}, we have
\[
J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m}{ \left( h_{\theta}(x^{i}) - y^{i}\right)^{2} } + \lambda \sum_{j=1}^{m} \theta_j^2 \right]
\]
N.B. we conventionally do not penalize $\theta_0$, so it is excluded from the sum.

The analytic solution is then
\[
\theta = \left( X^T X + \lambda 
\begin{bmatrix}
0 &  &  &  & \\ 
 & 1 &  &  & \\ 
 &  & 1 &  & \\ 
 &  &  & \ddots & \\ 
 &  &  &  & 1
\end{bmatrix}
\right)^{-1} X^T y
\]
which does not suffer from the problem of non-invertibility.

For \emph{logistic regression}, we have
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ - y \log(h_{\theta}(x^{(i)})) - (1 - y)(1 - \log(h_{\theta}(x^{(i)}))) \right] + \frac{\lambda}{2m} \sum_{j=1}^{m} \theta_j^2
\]

As with the learning rate, $\lambda$ must be appropriately chosen: too small and the regulation will have little effect, but too large will lead to \emph{underfitting}!

\section{Neural Networks}

The neuron model mimics the biological neuron: depending on some \emph{activation function} of the inputs (via \emph{dendrites}), the unit outputs (via \emph{axon}) some value (\emph{signal}). The \emph{neural network} simply connects such units in a sequence of layers, where the output of the unit in one layer is input for each unit in the next layer. 

The activation function is the sigmoid logistic function $g(z) = \frac{1}{1 + e^{-z}}$. Thus, each layer is a series of logistic regression models trained on the previous layer (to which we also add a \emph{bias unit} of constant value $1$).

The first layer is the actual \emph{input} to the model, the last layer the \emph{output}, while the intermediate layers are known as \emph{hidden layers}. See Figure \ref{nn-rep}\footnote{adapted from http://www.texample.net/tikz/examples/neural-network/}.

\begin{figure}
\centering
\caption{Representation of Neural Network}
\label{nn-rep}
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Nodes
    % Layer 1
    % Bias unit
    \node[input neuron, pin=left:Bias Unit] (I-0) at (0,0) {$x_0$};
    % Inputs
    \foreach \name / \y in {1,...,3}
        \node[input neuron, pin=left:Input Feature \y] (I-\name) at (0,-\y) {$x_\y$};

    % Layer 2
    % Bias unit
    \path[yshift=0.5cm] node[hidden neuron] (H-0) at (\layersep,0) {$a^{(2)}_0$};
    % Hidden layer
    \foreach \name / \y in {1,...,4}
        \path[yshift=0.5cm] node[hidden neuron] (H-\name) at (\layersep,-\y cm) {$a^{(2)}_\y$};

    % Layer 3
    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output $h_{\Theta}(x)$}, right of=H-2] (O) {$a^{(3)}_1$};


    % Edges
    % Layer 1 -> Layer 2
    \foreach \dest in {1,...,4}
        \path (I-0) edge [dashed] (H-\dest);
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,4}
            \path (I-\source) edge (H-\dest);
            
    % Layer 2 -> Layer 3
    \path (H-0) edge [dashed] (O);
    \foreach \source in {1,...,4}
        \path (H-\source) edge (O);

    % Layer labels
    \node[annot,above of=H-0, node distance=1cm] (hl) {Hidden layer};
    \node[annot,left of=hl] {Input layer};
    \node[annot,right of=hl] {Output layer};
\end{tikzpicture}
\end{figure}

\subsection{Forward propagation}
\label{fwd-prop}
\emph{Forward propagation} describes the method by which, given a trained neural network, we can compute predictions.

Consider a neural network with $L$ layers, with $s_j$ units in layer $j$.

Let $x_1, \ldots, x_{s_0}$ be the training data, to which we add $x_0 := 1$. Set $a^{(1)} := x$. 

Let $\Theta^{(j)}$ be the parameters (weights) of the mapping between layers $j$ and $(j+1)$, so that the $i\textsuperscript{th}$ row of $\Theta^{(j)}$ is the logistic regression parameters for the $i\textsuperscript{th}$ unit in layer $(j+1)$. Thus $\Theta^{(j)}$ is of dimension $s_{j+1} \times (s_{j} + 1)$.

The \emph{activations} of the units in layer $j$ is given by the $s_j$ dimensional vector $a^{(j)}$ where
\begin{align*}
z^{(j)} &= \Theta^{(j-1)} a^{(j-1)} \\
a^{(j)} &= g(z^{(j)})
\end{align*}
and at each layer we also add the bias unit $a_0^{(j)} := 0$. The output is $h_\Theta(x) = a^{(L)}$ (which may be multi-dimensional).

\subsection{Multiclass classification}
\label{nn-multiclass}
Given labels $\Set{1, \ldots, K}$, we model label $k$ as the unit coordinate vector for dimension $k$, i.e. a vector with $1$ in dimension $k$, and $0$ in all other dimensions.

For example, to represent $K = 3$ classes, we would use 
\[
1 \to \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},
2 \to \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \text{ and }
3 \to \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} 
\]

\subsection{Examples of neural networks}
As illustration of how neural networks can represent more advanced models than simple linear or logistic regression, we consider (approximate) representations of well-known binary operations.
\begin{ex}[AND]
With $x_1, x_2 \in \Set{0, 1}$ and $y = x_1 \land x_2$. 

We use a neural network with no hidden layer, and one output unit (i.e. simple logistic regression!). Take $\Theta^{(1)} = \begin{pmatrix} -30 & 20 & 20 \end{pmatrix}$.
\end{ex}

\begin{ex}[OR]
With notation above, but to represent $y = x_1 \lor x_2$, we can take $\Theta^{(1)} = \begin{pmatrix} -10 & 20 & 20 \end{pmatrix}$.
\end{ex}

\begin{ex}[NOT]
With $x_1 \in \Set{0, 1}$ and $y = \lnot x_1$, we can take $\Theta^{(1)} = \begin{pmatrix} 10 & -20 \end{pmatrix}$.
\end{ex}

\begin{ex}[XNOR]
We can represent $y = \lnot (x_1 \oplus x_2)$, by combining the above examples into a neural networks with one hidden layer, using

\[
\Theta^{(1)} = \begin{pmatrix} -30 & 20 & 20 \\ 10 & -20 & -20\end{pmatrix}
\]

and

\[
\Theta^{(2)} = \begin{pmatrix} -10 & 20 & 20\end{pmatrix}
\]

which is taking the hidden layer activations as $a_1^{(2)} = x_1 \land x_2$ and $a_2^{(2)} = (\lnot x_1) \land (\lnot x_2)$, and the output is $y = h_{\Theta}(x) = a_1^{(3)} = a_1^{(2)} \lor a_2^{(2)}$. See Figure \ref{xnor-rep} and Table \ref{xnor-net}.
\end{ex}

\begin{figure}
\centering
\caption{XNOR Neural Network}
\label{xnor-rep}
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
    \tikzstyle{edge label}=[near start, above, font=\footnotesize, inner sep=2pt];
	
    \def\nodesep{1.5 cm}
    % Nodes
    % Layer 1
    \node[neuron] (I-0) at (0,0) {$+1$};
    \foreach \name / \y in {1,2}
        \node[neuron] (I-\name) at (0,-\y * \nodesep) {$x_\y$};

    % Layer 2
    \node[neuron] (H-0) at (\layersep,0) {$+1$};
    \foreach \name / \y in {1,2}
        \node[neuron] (H-\name) at (\layersep,-\y * \nodesep) {$a^{(2)}_\y$};

    % Layer 3
    \node[neuron,pin={[pin edge={->}]right:$x_1 \oplus x_2$}, right of=H-1] (O) {$a^{(3)}_1$};

    % Edges
    % Theta 1
    \path (I-0) edge node[edge label] {$-30$} (H-1);
    \path (I-1) edge node[edge label] {$20$} (H-1);
    \path (I-2) edge node[edge label] {$20$} (H-1);
    
    \path (I-0) edge node[edge label] {$10$} (H-2);
    \path (I-1) edge node[edge label] {$-20$} (H-2);
    \path (I-2) edge node[edge label] {$-20$} (H-2);
    
    % Theta 2
    \path (H-0) edge node[edge label] {$-10$} (O);
    \path (H-1) edge node[edge label] {$20$} (O);
    \path (H-2) edge node[edge label] {$20$} (O);
\end{tikzpicture}
\end{figure}

\begin{table}[]
\centering
\caption{XNOR neural network}
\label{xnor-net}
\begin{tabular}{ccccc}
$x_1$ & $x_2$ & $a_1^{(2)}$ & $a_2^{(2)}$ & $h_{\Theta}(x)$ \\ 
\hline
0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 1 & 1 \\
\end{tabular}
\end{table}

\subsection{BackpropAgation}

\begin{defn}[Cost function for Neural Network]
Suppose we have $m$ training examples $x^{(i)}, \ldots, x^{(m)}$, and $K$ classes (so we represent the observed outputs as $y_i \in \Set{0, 1}^K$, as described in \S\ref{nn-multiclass}). Then the cost function is given by
\begin{align*}
J(\Theta) = &- \frac{1}{m} \left( \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log{h_{\Theta}(x^{(i)})_k} + (1 - y_k^{(i)}) \log{(1 - h_{\Theta}(x^{(i)})_k)} \right) \\
&+ \frac{\lambda}{2m} \sum_{l = 1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{i, j}^{(l)} )^2
\end{align*}
where $h_{\Theta}(x^{(i)})$ is the prediction as obtained by forward-propagation (described in \S\ref{fwd-prop}) for training example $i$, and $\lambda$ is the \emph{regularization parameter} (as in \S\ref{logistic-regularization}).
\end{defn}

The method of \emph{backpropagation} describes a way to compute the gradients $\frac{\partial J(\Theta)}{\partial \Theta_{ij}^{(l)}}$ so that we can apply an optimization algorithm (such as gradient-descent) to minimize $J(\Theta)$.

Intuitively, we run a forward pass (\S\ref{fwd-prop}) to compute the activations throughout the network, then for each node $j$ in layer $l$ we compute an \emph{error term} $\delta_j^{(l)}$ that measures how much that node was `responsible' for errors in the output. For the output layer, this can simply be the distance between $h(x)$ and $y$, while for the hidden layers we can take a weighted (by $\Theta$) averaged of the errors in the following layer.

\begin{defn}[Backpropagation Algorithm] 
For a given training example $x^{(t)}$, we compute the gradient as follows:

\begin{enumerate}
\item Set the input layer $a^{(1)} := x^{(t)}$.
\item Perform forward propagation to compute the activations $(z^{(j)}, a^{(j)})$ for layers $2, \ldots, L$.
\item For the output layer $L$, compute $\delta^{(L)} = a^{(L)} - y$, where $y$ is as in \S\ref{nn-multiclass}.
\item For layers $l = L-1, \ldots, 2$, compute $\delta^{(l)} = (\Theta^{(l)} )^T \delta^{(l+1)} \odot g'(z^{(l)})$, where $g'(z^{(l)}) = a^{(l)} \odot (1 - a^{(l)})$. \footnote{Here I am using the notation $\odot$ to mean element-wise multiplication (i.e. $.*$ in Matlab).}
\item Accumulate $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^T$.
\end{enumerate}
Finally, we have
\[
\frac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = 
\begin{cases}
\frac{1}{m} \left( \Delta_{i,j}^{(l)}  + \lambda \Theta_{i, j}^{(l)} \right) & \text{for } j \neq 0 \\
\frac{1}{m} \Delta_{i,j}^{(l)}  & \text{for } j = 0
\end{cases}
\]
\end{defn}

\subsubsection{Gradient checking}
It can be helpful in debugging to check the gradient computed in backpropagation against a numerical approximation, for example using the symmetric difference quotient $f(x+h)-f(x-h) \over 2h$ in each dimension of $\Theta$, and comparing to the gradient from backpropagation which should be similar.

\subsubsection{Random initialization}
The initial parameters must be randomly initialized because if otherwise we started at $\Theta = 0$ uniformly then by symmetry the parameters in each layer would remain equal on every iteration.

\end{document}